ls



\begin{document}
\noindent Author: Benjamin Smidt

\noindent Created: September 22, 2022

\noindent Last Updated: October 5th, 2022
\begin{center}
\section*{Deep Learning Resources}
\end{center}

\tableofcontents{}

\section{Word Vectors}
\begin{enumerate}
\item Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). 
    Efficient estimation of word representations in vector space. 
    \href{https://arxiv.org/abs/1301.3781}{CoRR, abs/1301.3781}. 
    [\emph{Word2Vec Algorithm}]
\item Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. P. 
    (2011). Natural language processing (almost) from scratch. 
    \href{https://arxiv.org/abs/1103.0398}{CoRR, abs/1103.0398}.
    [\emph{Transforming Words into Word Vectors}]
\end{enumerate}

\section{Data Preprocessing}

\subsection{Weight Initialization}

\section{Loss Functions}
\begin{enumerate}
\item Mikolov et al., 
    "Distributed Representations of Words and Phrases and their
    Compositionality", 
    \href{https://arxiv.org/abs/1310.4546}{ArXiv 2013}
    [\emph{Hierarchical Softmax (Problem of Many Classes)}]
\end{enumerate}


\section{Architectures}

\section{Hyperparameters}
\subsection{Update Rules}

\subsection{Regularization}

\subsection{Dropout}

\end{document}
