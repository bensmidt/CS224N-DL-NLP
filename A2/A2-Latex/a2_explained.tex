\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
% hyper links
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{bm}
% Formatting quotes properly
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{hyperref}
\usepackage{enumitem}



% \newif\ifanswers
% \answerstrue % comment out to hide answers

% \usepackage[compact]{titlesec}
% \usepackage{fancyhdr} % Required for custom headers
% \usepackage{lastpage} % Required to determine the last page for the footer
% \usepackage{extramarks} % Required for headers and footers
% \usepackage[usenames,dvipsnames]{color} % Required for custom colors
% \usepackage{graphicx} % Required to insert images
% \usepackage{listings} % Required for insertion of code
% \usepackage{courier} % Required for the courier font
% \usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
% \usepackage{enumerate}
% \usepackage{enumitem}
% \usepackage{subfigure}
% \usepackage{booktabs}
% \usepackage{amsmath, amsthm, amssymb}
% \usepackage{caption}
% \usepackage{hyperref}
% \captionsetup[table]{skip=4pt}
% \usepackage{framed}
% \usepackage{bm}
% \usepackage{minted}
% \usepackage{tikz}
% \usetikzlibrary{positioning,patterns,fit}

\begin{document}

\noindent Author: Benjamin Smidt

\noindent Created: October 3rd, 2022

\noindent Last Updated: October 5th, 2022
\begin{center}
\section*{CS 224N A2: Word Vectors}
\end{center}

\paragraph{} Note to the reader. This is my work for assignment one of Stanford's course
\href{https://web.stanford.edu/class/cs224n/}{CS 224N: Natural Language Processing with Deep Learning}. 
You can find the lecture Winter 2021 lectures series on YouTube \href{https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ}{here}.
This document is meant to be used as a reference, explanation, and resource for the assignment, 
not necessarily a comprehensive overview of Word Vectors. If there's a typo or a correction 
needs to be made, feel free to email me at benjamin.smidt@utexas.edu so I can fix it. 
Thank you! I hope you find this document helpful :). 

\tableofcontents{}

\newpage

\section{Written Understanding}

%% PROBLEM 1-A %%
\subsection{Problem A}
\underline{Instructions}
~\\
Prove that the naive-softmax loss is the same as the cross-entropy 
loss between $\bm y$ and $\hat{\bm y}$, i.e. (note that $\bm y, \hat{\bm y}$ 
are vectors and $\hat{\bm y}_o$ is a scalar).
~\\
~\\
\underline{Solution}
~\\
To start with, our naive-softmax loss is defined as
\begin{equation*}
    \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U) = -\log P(O=o| C=c)
\end{equation*}
where 
\begin{equation*}
P(O=o \mid C=c) = \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
Let's define our variables. $ \hat{\bm y}$ is our score vector. Note that the 
numerator is a vector of length $\bm V$ while the denominator is a scalar (this notation 
is a bit abusive but I think it actually makes things clearer). Thus, each index 
in the vector can be interpeted as the probability that the corresponding word (using the 
the index and one hot vector) is the center word. 

\begin{equation*}
    \hat{\bm y} = 
    \frac{\exp(\bm u^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
$\bm y$ is the one hot vector of the true center word. Then

\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    \bm -\bm y_1 \log(\hat{\bm y}_1) + ... + -\bm y_o \log(\hat{\bm y}_o) + ... + -\bm y_w \log(\hat{\bm y}_w)
\end{equation*}    
Where the index $\bm o$ indicates the index containing the only 1 within $\hat {\bm y}$ (since it is a one-hot vector). 
\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    \bm - (0)\log(\hat{\bm y}_1) + ... + -\bm (1) \log(\hat{\bm y}_o) + ... + - (0) \log(\hat{\bm y}_w)
\end{equation*}    
\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    -\log(\hat{\bm y}_o)
\end{equation*}    
\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    - \log(\frac{\exp(\bm u_o^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)})
\end{equation*}    
\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    - \log P(O=o| C=c)
\end{equation*}   
\begin{equation*} 
    -\sum_{w \in \text{Vocab}} \bm y_w \log(\hat{\bm y}_w) = 
    \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)
\end{equation*}     
I know the instructions said one line but I was going for clarity here. Obviously, I could not define
the variables (leave it to you to figure out what they mean) and just write some 
one liner that connects the dots. However, I wanted to make this as clear to understand as possible. 
Hopefully this leaves no room for ambiguity. 

%% PROBLEM 1-B %%
\subsection{Problem B}
\underline{Instructions}
~\\
Compute the partial derivative of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$ with respect to $\bm v_c$. 
Please write your answer in terms of $\bm y$, $\hat{\bm y}$, and $\bm U$. Additionally, answer the following 
two questions with one sentence each: (1) When is the gradient zero? (2) Why does subtracting this gradient, 
in the general case when it is nonzero, make $\bm v_c$ a more desirable vector (namely, a vector closer to 
outside word vectors in its window)?
~\\
~\\
\underline{Solution}
~\\
We start with our definition of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$.
\begin{equation*}
    \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U) = 
    -log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    \frac{\partial}{\partial \bm v_c} 
    - log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    \frac{\partial}{\partial \bm v_c} 
    - \bm u_{o}^\top \bm v_c +
    \frac{\partial}{\partial \bm v_c} 
    \; log \sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)
\end{equation*}
Everything up to this point should be easy to follow if you remember calculus (although see the 
first lecture where he goes through these exact steps in detail if you are confused). I then 
do a quick change of variables to ensure I know what I'm taking my partial derivative with respect to.
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \bm u_{o} +
    \frac{1}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
    \frac{\partial}{\partial \bm v_c} 
    {\sum_{j \in \text{Vocab}} \exp(\bm u_{j}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \bm u_{o} +
    \frac{\sum_{j \in \text{Vocab}} \bm u_{j}^\top \exp(\bm u_{j}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
From here I hope you can see that if you remove the $\bm u_j^\top$ form the second sum term 
you're left with $\bm {\hat y}$. Simplifying with this in mind we get the following.
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \bm U^\top \bm y + \bm U^\top \bm {\hat y_w}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    \bm U^\top (\bm {\hat y} - \bm y)
\end{equation*}
~\\
1. The gradient is zero when $\bm {\hat y} = \bm y$. Obviously if our predicted and correct vectors 
are equivalent then our accuracy is perfect and there's no update that could improve the loss.
~\\
2. Because we're doing gradient descent (as opposed to ascent), the update adds some portion ($\propto \alpha)$ of 
$\bm U^\top \bm y$ and subtracts some portion of $\bm U^\top \bm {\hat y}$. This makes 
intuitive sense because adding the correct vector $\bm u_o^\top$ makes it more like that vector (which is what we want) and 
subtracting by $\bm U^\top \bm {\hat y}$, the weighted average of our incorrect vectors that are 
producing the loss, makes it less like those vectors (again, what we want). 

%% PROBLEM 1-C %%
\subsection{Problem C}
\underline{Instructions}
~\\
Compute the partial derivatives of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$ with respect to 
each of the `outside' word vectors, $\bm u_w$'s. There will be two cases: when $w=o$, the true 
`outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of 
$\bm y$, $\hat{\bm y}$, and $\bm v_c$. In this subpart, you may use specific elements within these 
terms as well (such as $\bm y_1$, $\bm y_2$, $\dots$). Note that $\bm u_w$ is a vector while 
$\bm y_1, \bm y_2, \dots$ are scalars.
~\\
~\\
\underline{Solution}
~\\
\textbf{Case 1: $\bm u_w = u_o$}
~\\
We start with the case that $\bm u_w = u_o$. That is, the gradient with respect to the correct 
embedding vector. 
\begin{equation*}
    \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U) = 
    -log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    \frac{\partial}{\partial \bm u_o} 
    - log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    \frac{\partial}{\partial \bm u_o} 
    - log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    \frac{\partial}{\partial \bm u_o} 
    - \bm u_o^\top \bm v_c +
    \frac{\partial}{\partial \bm u_o} 
    \; log \sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    - \bm v_c +
    \frac{1}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
    \frac{\partial}{\partial \bm u_o} 
    \sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    - \bm v_c +
    \frac{\bm v_c \exp(\bm u_o^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    - \bm v_c + \bm v_c (\bm {\hat y} \cdot \bm y)
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    \bm v_c ( \bm {\hat y_0} - 1 )
\end{equation*}
Note that $\bm {\hat y} \cdot \bm y$ is the dot product of the two vectors. 
~\\
~\\
\textbf{Case 2: $\bm u_w = \bm u_o$}
~\\
Now we move onto the case that $\bm u_w \ne \bm u_o$. That is, the gradient with respect to 
any vector $\bm u_w$ that isn't $\bm u_o$. I'll use the notation $\bm u_j$ to indicate a 
specific $\bm u_w$ and (hopefully) prevent any confusion.
\begin{equation*}
    \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U) = 
    -log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \frac{\partial}{\partial \bm u_j} 
    - log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \frac{\partial}{\partial \bm u_j} 
    - log \frac{\exp(\bm u_{o}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \frac{\partial}{\partial \bm u_j} 
    - \bm u_o^\top \bm v_c +
    \frac{\partial}{\partial \bm u_j} 
    \; log \sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \frac{1}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
    \frac{\partial}{\partial \bm u_j} \sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \frac{\bm v_c \exp(\bm u_{j}^\top \bm v_c)}{\sum_{w \in \text{Vocab}} \exp(\bm u_{w}^\top \bm v_c)}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \bm v_c \; \bm {\hat y_j}
\end{equation*}

%% PROBLEM 1-D %%
\subsection{Problem D}
\underline{Instructions}
~\\
Write down the partial derivative of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$ 
with respect to $\bm U$. Please break down your answer in terms of 
$\frac{\partial \bm J(\bm v_c, o, \bm U)}{\partial \bm u_1}$, 
$\frac{\partial \bm J(\bm v_c, o, \bm U)}{\partial \bm u_2}$, 
$\cdots$, $\frac{\partial \bm J(\bm v_c, o, \bm U)}{\partial \bm u_{|\text{Vocab}|}}$. 
The solution should be one or two lines long.
~\\
~\\
\underline{Solution}
~\\
We already know 
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \bm v_c \; \bm {\hat y_j} \; \; \text{and} \; \; 
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_0} = 
    \bm v_c ( \bm {\hat y_0} - 1 )
\end{equation*}
Since $\bm y$ is a one hot vector, then
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_j} = 
    \bm v_c ( \bm {\hat y} - \bm y )
\end{equation*}
With that we can write
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm U} = 
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_1}, ..., 
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm u_V}
\end{equation*}
where $V$ is the index of the last word vector in $\bm U$. It follows then that 
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm U} = 
    \bm v_c \; \bm {\hat y_1}, 
    \bm v_c \; \bm {\hat y_2}
    , ..., 
    \bm v_c \; \bm {\hat y_V}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)}{\partial \bm U} = 
    \bm v_c ( \bm {\hat y} - \bm y )^\top
\end{equation*}
where we use the outer product to get the proper shape for $\bm U$. 

%% PROBLEM 1-E %%
\subsection{Problem E}
\underline{Instructions}
~\\
The ReLU (Rectified Linear Unit) activation function is given by the Equation:
\begin{equation}
    \label{ReLU}
    f(x) = \max(0, x)
\end{equation}
Please compute the derivative of $f(x)$ with respect to $x$, where $x$ is a scalar. 
You may ignore the case that the derivative is not defined at 0.
~\\
~\\
\underline{Solution}
~\\
We can break ReLU into two cases. 
\begin{equation*}
    f(x) = 
    \begin{cases}
        0 \; \; \text{if} \; \; x < 0 \\
        x \; \; \text{if} \; \; x > 0
    \end{cases}
\end{equation*}
Then the derivation becomes trivial.
\begin{equation*}
    f'(x) = 
    \begin{cases}
        0 \; \; \text{if} \; \; x < 0 \\
        1 \; \; \text{if} \; \; x > 0
    \end{cases}
\end{equation*}

%% PROBLEM 1-F %%
\subsection{Problem F}
\underline{Instructions}
~\\
The sigmoid function is given by:
\begin{equation*}
    \label{Sigmoid Function}
    \sigma (x) = \frac{1}{1 + e^{-x}} = \frac{e^{x}}{e^{x} + 1}
\end{equation*}
Please compute the derivative of $\sigma(x)$ with respect to $x$,
where $x$ is a scalar. Hint: you may want to write your answer in terms of $\sigma(x)$.
~\\
~\\
\underline{Solution}
~\\
\begin{equation*}
    \sigma (x) = \frac{1}{1 + e^{-x}} = \frac{e^{x}}{e^{x} + 1}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \frac{e^{x} (e^{x}+1) - e^{x}e^{x}}{(e^{x} + 1)^2}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \frac{e^{2x} + e^{x} - e^{2x}}{(e^{x} + 1)^2}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \frac{e^{x}}{(e^{x} + 1)^2}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \frac{e^{x}}{e^{x} + 1} \; \frac{1}{e^{x} + 1}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \frac{1}{1 + e^{-x}} \; \frac{1}{e^{x} + 1}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \sigma(x) \; \frac{1}{e^{x} + 1}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \sigma(x) \; \frac{e^{x} + 1 - e^{x}}{e^{x} + 1}
\end{equation*}
\begin{equation*}
    \sigma' (x) = \sigma(x) \; (\frac{e^x + 1}{e^x + 1} - \frac{e^{x}}{e^{x} + 1})
\end{equation*}
\begin{equation*}
    \sigma' (x) = \sigma(x) \; (1 - \frac{1}{1 + e^{-x}})
\end{equation*}\begin{equation*}
    \sigma' (x) = \sigma(x) \; (1 - \sigma(x))
\end{equation*}

%% PROBLEM 1-G %%
\subsection{Problem G}
\underline{Instructions}
~\\
Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss.  
Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we 
shall refer to them as $w_1, w_2, \dots, w_K$, and their outside vectors as 
$\bm u_{w_1}, \bm u_{w_2}, \dots, \bm u_{w_K}$. For this question, assume that the $K$ negative samples 
are distinct. In other words, $i\neq j$ implies $w_i\neq w_j$ for $i,j\in\{1,\dots,K\}$.
Note that $o\notin\{w_1, \dots, w_K\}$. 
For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:

\begin{equation*}
\bm J_{\text{neg-sample}}(\bm v_c, o, \bm U) = -\log(\sigma(\bm u_o^\top \bm v_c)) - \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
for a sample $w_1, \ldots w_K$, where $\sigma(\cdot)$ is the sigmoid function.
~\\

\subsubsection{(i)}
\underline{Instructions}
~\\
Please repeat parts (b) and (c), computing the partial derivatives of $\bm J_{\text{neg-sample}}$ 
with respect to $\bm v_c$, with respect to $\bm u_o$, and with respect to the $s^{th}$ negative sample 
$\bm u_{w_s}$. Please write your answers in terms of the vectors $\bm v_c$, $\bm u_o$, and $\bm u_{w_s}$, 
where $s \in [1, K]$. \textbf{Note:} you should be able to use your solution to part (f) to help compute the necessary gradients here.
~\\
~\\
\underline{Solution}
~\\
\emph{(b) Compute the partial derivative of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$ with respect to $\bm v_c$. 
Please write your answer in terms of $\bm y$, $\hat{\bm y}$, and $\bm U$. Additionally, answer the following 
two questions with one sentence each: (1) When is the gradient zero? (2) Why does subtracting this gradient, 
in the general case when it is nonzero, make $\bm v_c$ a more desirable vector (namely, a vector closer to 
outside word vectors in its window)?}
\begin{equation*}
    \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U) = -\log(\sigma(\bm u_o^\top \bm v_c)) - \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    \frac{\partial}{\partial \bm v_c}
    -\log(\sigma(\bm u_o^\top \bm v_c)) - 
    \frac{\partial}{\partial \bm v_c}
    \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \frac{1}{\sigma(\bm u_o^\top \bm v_c)} 
    \frac{\partial}{\partial \bm v_c}
    \sigma(\bm u_o^\top \bm v_c) - 
    \sum_{s=1}^K 
    \frac{\partial}{\partial \bm v_c}
    \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \frac{1}{\sigma(\bm u_o^\top \bm v_c)} 
    \frac{\partial}{\partial \bm v_c}
    \sigma(\bm u_o^\top \bm v_c) - 
    \sum_{s=1}^K \frac{1}{(\sigma(-\bm u_{w_s}^\top \bm v_c))}
    \frac{\partial}{\partial \bm v_c}
    (\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    - \frac{\sigma(\bm u_o^\top \bm v_c)}{\sigma(\bm u_o^\top \bm v_c)} 
    [1 - \sigma(\bm u_o^\top \bm v_c)] \bm u_o -
    \sum_{s=1}^K \frac{\sigma(-\bm u_{w_s}^\top \bm v_c)}{(\sigma(-\bm u_{w_s}^\top \bm v_c))}
    [1 - \sigma(-\bm u_{w_s}^\top \bm v_c)]
    (- \bm u_{w_s})
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K [1 - \sigma(-\bm u_{w_s}^\top \bm v_c)]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K [1 - \frac{1}{1 + \exp(-(-\bm u_{w_s}^\top \bm v_c))}]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K [\frac{1 + \exp(\bm u_{w_s}^\top \bm v_c)}{1 + \exp(\bm u_{w_s}^\top \bm v_c)}
    - \frac{1}{1 + \exp(\bm u_{w_s}^\top \bm v_c)}]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K [\frac{\exp(\bm u_{w_s}^\top \bm v_c)}{1 + \exp(\bm u_{w_s}^\top \bm v_c)}]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K [\frac{1}{1 + \exp(-\bm u_{w_s}^\top \bm v_c)}]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm u_o +
    \sum_{s=1}^K \sigma (\bm u_{w_s}^\top \bm v_c) \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm v_c} = 
    -[1 - \sigma(\bm u_o^\top \bm v_c)] \bm u_o +
    \sum_{s=1}^K \sigma (\bm u_{w_s}^\top \bm v_c) \bm u_{w_s}
\end{equation*}
You can see that the gradient is zero when 
$[1 - \sigma(\bm u_o^\top \bm v_c)] \bm u_o = \sum_{s=1}^K \sigma (\bm u_o^\top \bm v_c) \bm u_{w_s}$. 
For our first term, if $\bm u_o^\top \bm v_c$ is small (dissimilar) then $\sigma (\bm u_o^\top \bm v_c)$ evaluates to a large number.
Thus, in our gradient descent update, we'll be adding $\bm C \bm u_o$ ($\bm C$ = 1 - $\sigma (\bm u_o^\top \bm v_c)$) to 
$\bm v_c$. Intuitively this makes sense because adding $\bm C \bm u_o$ to $\bm v_c$ will make $\bm v_c$ 
more like $\bm u_o$, which is what we want.
~\\
~\\
If $\bm u_o^\top \bm v_c$ is large (similar) then $\sigma (\bm u_o^\top \bm v_c)$ evaluates to a small number 
and our gradient update for the first time adds approximately zero. Again, this is intuitive because if the 
vectors are similar already then we don't need to change $v_c$ much. 
~\\
~\\
For our second term, a similar line of reasoning can be invoked to see that we're subtracting out 
$\bm C \bm u_{w_s}$ from $\bm v_c$ if $\sigma (\bm u_{w_s}^\top \bm v_c)$ is large and doing nothing 
if $\sigma (\bm u_{w_s}^\top \bm v_c)$ is small. Again, this makes sense because we want $\bm v_c$ to be less 
like the vectors that aren't $\bm u_o^\top$ and don't need to change $\bm v_c$ if that's already the case. 

~\\
~\\
\emph{(c) Compute the partial derivatives of $\bm J_{\text{naive-softmax}}(\bm v_c, o, \bm U)$ with respect to 
each of the `outside' word vectors, $\bm u_w$'s. There will be two cases: when $w=o$, the true 
`outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of 
$\bm y$, $\hat{\bm y}$, and $\bm v_c$. In this subpart, you may use specific elements within these 
terms as well (such as $\bm y_1$, $\bm y_2$, $\dots$). Note that $\bm u_w$ is a vector while 
$\bm y_1, \bm y_2, \dots$ are scalars.}
~\\
~\\
\textbf{Case 1: $\frac{\partial}{\partial \bm u_o}$}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    \frac{\partial}{\partial \bm u_o}
    -\log(\sigma(\bm u_o^\top \bm v_c)) - 
    \frac{\partial}{\partial \bm u_o}
    \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    - \frac{\sigma(\bm u_o^\top \bm v_c)}{\sigma(\bm u_o^\top \bm v_c)}
    [1 - \sigma(\bm u_o^\top \bm v_c)] \bm v_c
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_o} = 
    [\sigma(\bm u_o^\top \bm v_c) - 1] \bm v_c
\end{equation*}
\textbf{Case 2: $\frac{\partial}{\partial \bm u_{w_s}}$}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_{w_s}} = 
    \frac{\partial}{\partial \bm u_{w_s}}
    -\log(\sigma(\bm u_o^\top \bm v_c)) - 
    \frac{\partial}{\partial \bm u_{w_s}}
    \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_{w_s}} = 
    - \frac{\sigma(-\bm u_{w_s}^\top \bm v_c)}{\sigma(-\bm u_{w_s}^\top \bm v_c)}
    [1 - \sigma(-\bm u_{w_s}^\top \bm v_c)]
    (- \bm u_{w_s})
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_{w_s}} = 
    [1 - \sigma(-\bm u_{w_s}^\top \bm v_c)]
    \bm u_{w_s}
\end{equation*}
\begin{equation*}
    \frac{\partial \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U)}{\partial \bm u_{w_s}} = 
    \sigma(\bm u_{w_s}^\top \bm v_c)    \bm u_{w_s}
\end{equation*}
See part (b) of this problem for that last jump, it's the exact same as before. 

    
\subsubsection{(ii)}
\underline{Instructions}
~\\
In lecture, we learned that an efficient implementation of backpropagation leverages the re-use of 
previously-computed partial derivatives. Which quantity could you reuse between the three partial derivatives 
to minimize duplicate computation? Write your answer in terms of 
$\bm{U}_{o, \{w_1, \dots, w_K\}} = \begin{bmatrix} \bm{u}_o, -\bm{u}_{w_1}, \dots, -\bm{u}_{w_K} \end{bmatrix}$, 
a matrix with the outside vectors stacked as columns, and $\bm{1}$, a $(K + 1) \times 1$ vector of 1's. 
~\\
~\\
\underline{Solution}
~\\
Since we compute the sigmoid function so much we could reuse the following: 
\begin{equation*}
    \sigma (\bm U_{o}^{\top} \bm v_c) - \bm 1
\end{equation*}
\subsubsection{(iii)}
\underline{Instructions}
~\\
Describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.

Caveat: So far we have looked at re-using quantities and approximating softmax with sampling for faster 
gradient descent. Do note that some of these optimizations might not be necessary on modern GPUs and are, 
to some extent, artifacts of the limited compute resources available at the time when these algorithms were developed.
~\\
~\\
\underline{Solution}
~\\
This loss function is much more efficient because, provided we have enough negative samples, the negative 
sampling we use will approximate the gradient update we would've gotten for the entire vocabulary but 
with literally a fraction of the vocabulary used. Instead of iterating through the entire vocabulary we 
can just iterate though negative samples which can be orders of magnitude smaller in size. 

\subsection{Problem H}
\underline{Instructions}
~\\
Now we will repeat the previous exercise, but without the assumption that the $K$ sampled words are distinct.  
Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall 
refer to them as $w_1, w_2, \dots, w_K$ and their outside vectors as $\bm u_{w_1}, \dots, \bm u_{w_K}$. 
In this question, you may not assume that the words are distinct. In other words, $w_i=w_j$ may be true 
when $i\neq j$ is true. Note that $o\notin\{w_1, \dots, w_K\}$. 
For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:

\begin{equation}
    \bm J_{\text{neg-sample}}(\bm v_c, o, \bm U) = -\log(\sigma(\bm u_o^\top \bm v_c)) - \sum_{s=1}^K \log(\sigma(-\bm u_{w_s}^\top \bm v_c))
    \end{equation}
for a sample $w_1, \ldots w_K$, where $\sigma(\cdot)$ is the sigmoid function.
~\\
~\\
Compute the partial derivative of $\bm J_{\text{neg-sample}}$ with respect to a negative sample $\bm u_{w_s}$. 
Please write your answers in terms of the vectors $\bm v_c$ and $\bm u_{w_s}$, where $s \in [1, K]$. 
Hint: break up the sum in the loss function into two sums: a sum over all sampled words equal to $w_s$ 
and a sum over all sampled words not equal to $w_s$. Notation-wise, you may write `equal' and `not equal' 
conditions below the summation symbols.    

~\\
~\\
\underline{Solution}
~\\











\end{document}